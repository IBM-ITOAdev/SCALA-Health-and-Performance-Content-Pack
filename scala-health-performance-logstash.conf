##################################################
# Logstash for SCALA Health and Performance
#
# v1.0 	7/1/14	Doug McClure
#
###############################################
#
# SCALA Log Types
# 
#  	type => "SCALA-GR" 	: GenericReceiver.log
#	type => "SCALA-UA" : UnityApplication.log
#	type => "SCALA-EIFR" : UnityEIFReceiver.log
#	type => "SCALA-SOLR" : solr.log
#	type => "SCALA-MANAGE-SOLR-NODES" : ManageSolrnodes.log
#	type => "SCALA-ZOOKEEPER" : zookeeper.out
#
#	TODO:
#	$SCALAHOME/logs/derby.log
#	$SCALAHOME/solr-4.7.1/scala_instance1/logs/2014_12_09-185000081.start.log ??
#	Distributed EIF Receiver logs
#	Distributed Solr logs
#
# Derived Types from logs
#
# GRSTATS : GenericReceiver BatchStatus messages - metrics: batchSize, numberSuccessful, numberFailures, indexedSourceVolume
# UASTATS : UnityApplication.log Query messages - metrics : queryResults, totalResults, queryTime
# EIFRSTATS : UnityEifReceiver.log Batch messages - metrics : batchSize, failures, indexNumFailures, indexNumSuccessful, indexMetadataVolume, numFailures, numSuccessful
#
###############################################
# NOTES:
#
# 1. Determine which SCALA log files you want to make use of. Some are more practical than others such as the GenericReceiver and UnityApplication logs.
# 2. Update input section based upon how you will bring in SCALA log files into logstash.  There are many options available to do this including the use of rsyslog/syslog-ng, logstash-forwarder, LFA or file inputs as shown below.
# 3. Update the patterns_dir parameter in each multiline and grok filter based on where you've placed the SCALAPATTERNS grok pattern file.
# 4. Ensure that you set your host and path as needed for aggregation and consolidation in your SCALA datasource. 
# 5. Set a "final" tag to indicate all processing is done and message is ready to ship to SCALA via output
#
#
###############################################

input {

	file {
		path => "/opt/scala/driver/logs/GenericReceiver.log"
		type => "SCALA-GR"
	}

	file {
		path => "/opt/scala/driver/logs/UnityApplication.log"
		type => "SCALA-UA"
	}

	file {
		path => "/opt/scala/driver/logs/UnityEifReceiver.log"
		type => "SCALA-EIFR"
	}	
	
	file {
		path => "/opt/scala/driver/solr-4.7.1/scala_instance1/logs/solr.log"
		type => "SCALA-SOLR"
	}
	
	file {
		path => "/opt/scala/driver/logs/ManangeSolrnodes.log"
		type => "SCALA-MANAGE-SOLR-NODES"
	}
	
	file {
		path => "/opt/scala/driver/logs/zookeeper.out"
		type => "SCALA-ZOOKEEPER"
	}
	
} #end inputs

###################

filter {

######################

if [type] == "SCALA-GR" {
	multiline {
		pattern => "^%{LATIMESTAMP}"
		patterns_dir => ["/opt/logstash150b1/patterns"]
		negate => "true"
		what => "previous"
	} #end multi

	grok {
		match => [ "message", "%{LALOGMSGALL}" ]
		patterns_dir => ["/opt/logstash150b1/patterns"]
		add_tag => [ "GR-Grokked" ]
	} #main grok

} #end 

#get batch status metrics from GenericReceiver.log

if [processName] == "UnityFlowController" {
	if [logMessage] =~ /.*Batch Status.*/ {
		grok {
			match => [ "logMessage","%{BATCHSTATUS}" ]
		patterns_dir => ["/opt/logstash150b1/patterns"]
			add_tag => [ "BATCHSTATUS-Grokked" ]
		}
	} #end conditional
		
	if "BATCHSTATUS-Grokked" in [tags] {
		mutate {
			replace => [ "type", "GRSTATS" ]
			add_tag => [ "BATCHSTATUS-Final" ]
			
			#convert to integers if you want to send to a es/kibana dashboard - still have to set them for DSV
			convert => [ "batchSize", "integer" ]
			convert => [ "numberSuccessful", "integer" ]
			convert => [ "numberFailures", "integer" ]
			convert => [ "indexedSourceVolume", "integer" ]
		} #end mutate
	} #end GR Batch Status conditional
} #end UnityFlowController Conditional

if "GR-Grokked" in [tags] and "BATCHSTATUS-Final" not in [tags] {
	date {
		match => [ "timestampOrig", "MM/dd/YY HH:mm:ss:SSS z" ]
	} #end date
	mutate {
		replace => [ "host", "SCALA-GR", "path", "SCALA-GR"]
		add_tag => ["SCALA-GR-final"]
	} #end mutate
} #end GR Main conditional

if "BATCHSTATUS-Final" in [tags] {
	date {
		match => [ "timestampOrig", "MM/dd/YY HH:mm:ss:SSS z" ]
	} #end date
	mutate {
		replace => [ "host", "SCALA-GRSTATS", "path", "SCALA-GRSTATS"]
		add_tag => ["SCALA-GRSTATS-final"]
	} #end mutate
} #end GR Main conditional


####only with GR in DEBUG####

if [processName] == "DataCollectorRestServlet" {
	if [logMessage] =~ /.*POST Result.*/ {
		grok {
			match => [ "logMessage","%{DCRS}" ]
		patterns_dir => ["/opt/logstash150b1/patterns"]
			add_tag => [ "RestAPI-POST-Grokked" ]
		}
		if "RestAPI-POST-Grokked" in [tags] {
			json {
				source => "postResult"
				add_tag => "JSON-Exploded"
			} #end json
		} #end conditional 
	} #end conditional
} #end

if "JSON-Exploded" in [tags] {
	date {
		match => [ "timestampOrig", "MM/dd/YY HH:mm:ss:SSS z" ]
	}	
	mutate {
		replace => [ "host", "SCALA-GR-STATS", "path", "SCALA-GR-STATS" ]
		add_tag => ["SCALA-GR-STATS-final"]
	} #end mutate
} #end conditional

####only with GR in DEBUG####


##############################################

if [type] == "SCALA-UA" {

	multiline {
		pattern => "^%{LATIMESTAMP}"
		patterns_dir => ["/opt/logstash150b1/patterns"]
		negate => "true"
		what => "previous"
	} #end multi

	grok {
		match => [ "message", "%{LALOGMSGALL}" ]
		patterns_dir => ["/opt/logstash150b1/patterns"]
		add_tag => [ "UA-Grokked" ]
	} #main grok

} #end
	
	
# this will parse out various search query related metrics. The issue is these are scattered 
# across five different log message patterns and not all in one message.  This makes using them
# difficult within es/kibana or scala. It's easier to use just one at a time.
	
if [processName] == "SolrSearchQuery" {
	grok {
		match => [ "logMessage","%{NUMRSLT}" ]
		match => [ "logMessage","%{TOTRSLT}" ]
		match => [ "logMessage","%{SEARCHTIME}" ]
		match => [ "logMessage","%{FILTERALL}" ]
		match => [ "logMessage","%{QUERY}" ]
		patterns_dir => ["/opt/logstash150b1/patterns"]
		add_tag => [ "SolrSearchQuery-Grokked" ]
	} #end grok
	
	if "SolrSearchQuery-Grokked" in [tags] {
		mutate {
			replace => [ "type", "UASTATS" ]
			add_tag => [ "SolrSearchQuery-Final" ]
			
			#convert to integers for es/kibana - still have to set them for DSV
			convert => [ "queryResults", "integer" ]
			convert => [ "totalResults", "integer" ]
			convert => [ "queryTime", "integer" ]
		} #end mutate
	} #end metric conditional	
}	#end SolrSearchQuery conditional

if "UA-Grokked" in [tags] and "SolrSearchQuery-Final" not in [tags] {
	date {
		match => [ "timestampOrig", "MM/dd/YY HH:mm:ss:SSS z" ]
	} #end date
	mutate {
		replace => [ "host", "SCALA-UA", "path", "SCALA-UA"]
		add_tag => ["SCALA-UA-final"]
	} #end mutate
} #end UA Main conditional

if "SolrSearchQuery-Final" in [tags] {
	date {
		match => [ "timestampOrig", "MM/dd/YY HH:mm:ss:SSS z" ]
	}	
	mutate {
		replace => [ "host", "SCALA-UA-STATS", "path", "SCALA-UA-STATS" ]
		add_tag => ["SCALA-UA-STATS-final"]
	} #end mutate
} #end conditional
	
##############################################

if [type] == "SCALA-EIFR" {

	multiline {
		pattern => "^%{LATIMESTAMP}"
		patterns_dir => ["/opt/logstash150b1/patterns"]
		negate => "true"
		what => "previous"
	} #end multi

	grok {
		match => [ "message", "%{LALOGMSGALL}" ]
		patterns_dir => ["/opt/logstash150b1/patterns"]
		add_tag => [ "EIFR-Grokked" ]
	} #main grok

if [processName] == "LogEventPoster" {
	if [logMessage] =~ /^\{/ {
		json {
			source => "logMessage"
			add_tag => "EIFR-JSON-Exploded"
		} #end json
	}

} #end

if "EIFR-JSON-Exploded" in [tags] {
		mutate {
			replace => [ "type", "EIFRSTATS" ]
			add_tag => "EIFR-STATS-final"
			
			#convert to integers for es/kibana - still have to set them for DSV
			
			convert => [ "[BATCH_STATUS][batchSize]", "integer" ]
			convert => [ "[BATCH_STATUS][failures]", "integer" ]
			convert => [ "[BATCH_STATUS][indexNumFailures]", "integer" ]
			convert => [ "[BATCH_STATUS][indexNumSuccessful]", "integer" ]
			convert => [ "[BATCH_STATUS][indexedMetadataVolume]", "integer" ]
			convert => [ "[BATCH_STATUS][indexedSourceVolume]", "integer" ]
			convert => [ "[BATCH_STATUS][numFailures]", "integer" ]
			convert => [ "[BATCH_STATUS][numSuccessful]", "integer" ]
			
		} #end mutate
	} #end EIFR Metrics	
} #end LogEventPoster	
	
if "EIFR-Grokked" in [tags] and "EIFR-STATS-final" not in [tags] {
	date {
		match => [ "timestampOrig", "MM/dd/YY HH:mm:ss:SSS z" ]
	} #end date
	mutate {
		replace => [ "host", "SCALA-EIFR", "path", "SCALA-EIFR"]
		add_tag => ["SCALA-EIFR-final"]
	} #end mutate
} #end UA Main conditional

if "EIFR-STATS-final" in [tags] {
	date {
		match => [ "timestampOrig", "MM/dd/YY HH:mm:ss:SSS z" ]
	}	
	mutate {
		replace => [ "host", "SCALA-EIFR-STATS", "path", "SCALA-EIFR-STATS" ]
		add_tag => ["SCALA-EIFR-STATS-final"]
	} #end mutate
} #end conditional	
		
############################################

#SCALA-SOLR	
	
if [type] == "SCALA-SOLR" {

	multiline {
		pattern => "^%{SOLRGENERIC}"
		patterns_dir => ["/opt/logstash150b1/patterns"]
		negate => "true"
		what => "previous"
	} #end multi

	grok {
		match => [ "message", "%{SOLRALL}" ]
		patterns_dir => ["/opt/logstash150b1/patterns"]
		add_tag => [ "SCALA-SOLR-Grokked" ]
	} #main grok

} #end SCALA-SOLR

if "SCALA-SOLR-Grokked" in [tags] {

#take timestampOrig (missing timezone) and convert it to @timestamp in proper ISO8601 format with timezone

	date {
		match => [ "timestampOrig", "YYYY-MM-dd HH:mm:ss.SSS", "ISO8601" ]
		target => "@timestamp"
	} #end date
	
	mutate {
		replace => [ "host", "SCALA-SOLR", "path", "SCALA-SOLR"]
		add_tag => ["SCALA-SOLR-final"]
	} #end mutate
} #end SCALA SOLR conditional

############################################

if [type] == "SCALA-MANAGE-SOLR-NODES" {

	multiline {
		pattern => "^%{MANAGESOLRNODETS}"
		patterns_dir => ["/opt/logstash150b1/patterns"]
		negate => "true"
		what => "previous"
	} #end multi

	grok {
		match => [ "message", "%{MANAGESOLRNODEALL}" ]
		patterns_dir => ["/opt/logstash150b1/patterns"]
		add_tag => [ "SCALA-MANAGE-SOLR-NODES-Grokked" ]
	} #main grok

} #end SCALA-MANAGE-SOLR-NODES

if "SCALA-MANAGE-SOLR-NODES-Grokked" in [tags] {
	
	#take timestampOrig (missing timezone) and convert it to @timestamp in proper ISO8601 format with timezone
	
	date {
		match => [ "timestampOrig", "YYYY-MM-dd HH:mm:ss.SSS", "ISO8601" ]
		target => "@timestamp"
	} #end date
	
	mutate {
		replace => [ "host", "SCALA-SOLR-NODES", "path", "SCALA-SOLR-NODES"]
		add_tag => ["SCALA-SOLR-NODES-final"]
	} #end mutate
} #end SCALA SOLR NODES conditional

############################################

if [type] == "SCALA-ZOOKEEPER" {
	
	multiline {
		pattern => "^%{ZOOKEEPERTS}"
		patterns_dir => ["/opt/logstash150b1/patterns"]
		negate => "true"
		what => "previous"
	} #end multi

	grok {
		match => [ "message", "%{ZOOKEEPERALL}" ]
		patterns_dir => ["/opt/logstash150b1/patterns"]
		add_tag => [ "SCALA-ZOOKEEPER-Grokked" ]
	} #main grok

} #end SCALA-ZOOKEEPER

if "SCALA-ZOOKEEPER-Grokked" in [tags] {
	
	#take timestampOrig (missing timezone) and convert it to @timestamp in proper ISO8601 format with timezone
	
	date {
		match => [ "timestampOrig", "YYYY-MM-dd HH:mm:ss,SSS", "ISO8601" ]
		target => "@timestamp"
	} #end date
	
	mutate {
		replace => [ "host", "SCALA-ZOOKEEPER", "path", "SCALA-ZOOKEEPER"]
		add_tag => ["SCALA-ZOOKEEPER-final"]
	} #end mutate
} #end SCALA SOLR NODES conditional

##############################################
	
} #end filters

output {

###############################################	
#
#for new logstash scala outuput 

if "SCALA-GR-final" in [tags] or "SCALA-GRSTATS-final" in [tags] or "SCALA-UA-final" in [tags] or "SCALA-UA-STATS-final" in [tags] or "SCALA-EIFR-final" in [tags] or "SCALA-EIFR-STATS-final" in [tags] or "SCALA-SOLR-final" in [tags] or "SCALA-SOLR-NODES-final" in [tags] or "SCALA-ZOOKEEPER-final" in [tags] or "SCALA-EIFR-STATS" in [tags] {
	scala { 
		scala_url => "https://10.0.0.1:9987/Unity/DataCollector"
		scala_user => "unityadmin" 
		scala_password => "unityadmin" 
		batch_size => 500000
		idle_flush_time => 5 
		sequential_flush => true
		num_concurrent_writers => 5
		use_structured_api => true
		disk_cache_path => "/opt/logstash/cache/tmp_spool_dir_SCALA"
				
		scala_fields => {
			"SCALA-GR@SCALA-GR,SCALA-UA@SCALA-UA,SCALA-EIFR@SCALA-EIFR" => "timestampOrig,threadID,logLevel,processName,logMessage"
			"SCALA-GRSTATS@SCALA-GRSTATS" => "timestampOrig,collectionName,batchSize,numberSuccessful,numberFailures,indexedSourceVolume"
			"SCALA-UA-STATS@SCALA-UA-STATS" => "@timestamp,query,queryNumber,queryResults,totalResults,queryTime,datasources,timerange"
			"SCALA-EIFR-STATS@SCALA-EIFR-STATS" => "@timestamp,[BATCH_STATUS][batchSize],[BATCH_STATUS][failures],[BATCH_STATUS][indexNumFailures],[BATCH_STATUS][indexNumSuccessful],[BATCH_STATUS][indexedMetadataVolume],[BATCH_STATUS][indexedSourceVolume],[BATCH_STATUS][numFailures],[BATCH_STATUS][numSuccessful]"
			"SCALA-SOLR@SCALA-SOLR" => "@timestamp,logLevel,logMessage"
			"SCALA-SOLR-NODES@SCALA-SOLR-NODES" => "@timestamp,logLevel,processName,logMessage"
			"SCALA-ZOOKEEPER@SCALA-ZOOKEEPER" => "@timestamp,ID,logLevel,processName,logMessage"
		} #end scala_fields
		date_format_string => "yyyy-MM-dd'T'HH:mm:ssX"
	} #end scala output
} #end scala output conditional


################################
# for debugging
################################

stdout {
	codec => rubydebug
	}

if "_grokparsefailure" in [tags] {
	file {
		codec => rubydebug
		path => "/opt/logstash/debug.log"
	} # end file 
} #end conditional

} #end outputs
